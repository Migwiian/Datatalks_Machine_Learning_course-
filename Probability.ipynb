{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5f9e29fe-bbcb-45b8-9e2f-053c0ef517cb",
   "metadata": {},
   "source": [
    "# Part 1: The Math of Probability (The Language)\n",
    "Logistic Regression's goal is to output a probability. To understand how it does that, we first need to understand a few key ways of thinking about probability."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9316bfd4-fda9-41a4-9b16-807ffd45dbc9",
   "metadata": {},
   "source": [
    "## Probability (P)\n",
    "This is the most familiar concept. It's a number between 0 and 1 that represents the likelihood of an event.\n",
    "\n",
    "* P = 0.8 means there is an 80% chance of something happening.\n",
    "* P = 0.5 means a 50/50 chance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b29cabc-1cb8-4af8-863a-c9a7666ae3b3",
   "metadata": {},
   "source": [
    "## Odds\n",
    "Odds are another way to express likelihood, often used in gambling. It's a ratio of the event happening to the event not happening.\n",
    "\n",
    "Formula: Odds = P / (1 - P)\n",
    "\n",
    "Example:\n",
    "\n",
    "* If there's an 80% chance of rain (P = 0.8), the odds of rain are:\n",
    "  Odds = 0.8 / (1 - 0.8) = 0.8 / 0.2 = 4.\n",
    "    We say the odds are \"4 to 1\". For every 1 time it doesn't rain, it rains 4 times.\n",
    "* If there's a 50% chance (P = 0.5), the odds are 0.5 / 0.5 = 1. The odds are \"1 to 1\"."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34bc493d-7d7f-4b77-897b-91082505cbee",
   "metadata": {},
   "source": [
    "## Log-Odds (The Logit)\n",
    "This is the most important concept for understanding the math of Logistic Regression.\n",
    "\n",
    "What if we take the natural logarithm (ln) of the odds?\n",
    "\n",
    "__Formula__: Log-Odds = ln(Odds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b59f2435-4b98-40d4-a67a-03fd6808f86d",
   "metadata": {},
   "source": [
    "__Example__\n",
    "\n",
    "* For our 80% chance of rain, the odds were 4. The log-odds are ln(4) ≈ 1.39.\n",
    "* For a 50% chance, the odds were 1. The log-odds are ln(1) = 0.\n",
    "* What if the probability is only 20% (P = 0.2)? The odds are 0.2 / 0.8 = 0.25. The log-odds are ln(0.25) ≈ -1.39"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ff7354a-95fc-4939-93fb-72920bfe3069",
   "metadata": {},
   "source": [
    "__Aha! Moment:__ Look at the scale of log-odds:\n",
    "\n",
    "* When probability is less than 50%, log-odds are negative.\n",
    "* When probability is exactly 50%, log-odds are zero.\n",
    "* When probability is greater than 50%, log-odds are positive.\n",
    "* The log-odds scale is unbounded in both directions (-infinity to +infinity). This is the perfect scale for a linear model to work with."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "710b839a-e161-47bd-8e48-2fa74c66aad3",
   "metadata": {},
   "source": [
    "# Part 2: The Math of Logistic Regression (The Model)\n",
    "Now we can build the model. Logistic Regression is essentially a two-step process: a linear step and a non-linear \"squishing\" step."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "826a2454-6d2b-48c1-9758-287a821a6074",
   "metadata": {},
   "source": [
    "## Step 1: The Linear Part (Calculating a \"Score\")\n",
    "This part is exactly the same as Linear Regression. The model calculates a weighted sum of the input features.\n",
    "\n",
    "__Formula:__ Score = (w₁x₁) + (w₂x₂) + ... + (wₙxₙ) + b\n",
    "\n",
    "* x₁, x₂, ... are your features (e.g., tenure, monthly_charges).\n",
    "* w₁, w₂, ... are the weights (or coefficients) the model learns. A large positive weight means the feature has a strong positive influence on the outcome.\n",
    "b is the bias (or intercept). It's the baseline score when all features are zero.\n",
    "* This Score can be any number, from very negative to very positive.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29b09e2f-e099-4e8f-864e-474cc0c3f300",
   "metadata": {},
   "source": [
    "* The Core Assumption of Logistic Regression: This Score we just calculated is a direct estimate of the log-odds of the event.\n",
    "\n",
    "* Log-Odds = Score = (w₁x₁) + (w₂x₂) + ... + b\n",
    "\n",
    "* This is the brilliant link. We've connected our linear equation to the world of probability."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a61f60c-d577-430e-b785-ca525679afd8",
   "metadata": {},
   "source": [
    "# Step 2: The Non-Linear \"Squishing\" Part (The Sigmoid Function)\n",
    "We have the log-odds, but we want a final probability between 0 and 1. We just need to reverse our steps from Part 1.\n",
    "\n",
    "1. __From Log-Odds to Odds:__ We reverse the logarithm with exponentiation.\n",
    "Odds = e^(Score)\n",
    "2. __From Odds to Probability:__ We use the odds formula from before.\n",
    "P = Odds / (1 + Odds)\n",
    "\n",
    "If you combine these two steps into a single formula, you get the famous __Sigmoid Function:__"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32d02ae7-8cbf-4820-9d92-dab921c37ccc",
   "metadata": {},
   "source": [
    "__Formula:__ P = 1 / (1 + e^(-Score))\n",
    "\n",
    "This function takes any Score (from -infinity to +infinity) and elegantly \"squishes\" it into a probability between 0 and 1.\n",
    "\n",
    "If the Score is a large positive number, e^(-Score) becomes tiny, so P approaches 1.\n",
    "If the Score is zero, e^(-Score) is 1, so P = 1 / (1 + 1) = 0.5.\n",
    "If the Score is a large negative number, e^(-Score) becomes huge, so P approaches 0."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
